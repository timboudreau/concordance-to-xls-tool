Concordance To XLSX (or whatever)
=================================

A quick and dirty Java GUI application that can take pairs of (or a whole folder of pairs of) `.dat` + `.opt` files
generated by (at least one version of) [LexisNexis Concordance](https://en.wikipedia.org/wiki/LexisNexis) crude legal
"database" - a format used by lawyers, it seems, particularly when they must comply with providing discovery, but
do so in a way that the recipient won't be able to use - as in, you are given a zip file with a bunch of 
pairs of `.opt` and `.dat` files, which contain metadata about a gazillion raw files of random types.

Build the project `maven`, then simply run it with `java -jar`.  A Swing UI will open, in which you:

  * Point it at a folder containing one or more pairs of `.opt` + `.dat` files with the same name - it
  can either scan all subfolders recursively, or just look in the one folder
  * Choose some words or phrases that, if present in a record, exclude it from the output
  * Choose an output format
    * CSV - vanilla comma-separated values
    * JSON - a JSON file with a giant array of all records
    * XSLX - spreadsheet created using Apache Poi loadable in Excel or OpenOffice
      * Poi is a bit naive about memory requirements, and expects to be able to hold the entire universe in memory,
        which isn't likely when you have 100Gb of data.  The CSV option is a better choice for huge data-sets.
      * Either Poi, or perhaps the Excel format is limited to 32,767 characters per cell - longer values are truncated
  * Choose an output file and perform the conversion

The resulting file will contain (modulo filtering) all of the data from the `.dat` metadata file in a form readable by
mere mortals, _with the file path for where the data actually is_ and the name of the "volume" (non-numeric prefix portion of
the input file names), so when you find a row with something interesting, you can actually figure out where to find it
on disk.

The synthesized column names:

  * `Volume` - the name of the opt/dat file pair the record was found in
  * The source column name with `.File` appended to it.  These matches are found by pattern matching the volume name and the cell
  value - i.e. if you have FOOVOL00001 for your file name, and encounter a value FOO000023, we check our `.opt` file dictionary
  for a file path that maps to the volume we're in with the cell value, and if there is a match, synthesize a cell
  `$ORIGINAL_CELL_NAME.File` to include in each record, so that you can map every metadata record to the file containing
  the data in question.

Why?
----

A friend of mine is involved in a lawsuit, and was entitled to obtain the discovery produced by opposing counsel.  They
provided it - decades more than was needed of it in a format only readable by that expensive and ancient software, as
a clever way to comply with the request, providing about 80Gb (zipped!) of 99.9% useless material in a form she
could not search.

I mean, if you have decades worth of emails, the natural way to work with that is to turn them all into _images_
and then OCR them, right?

Well, that's not terribly cool.

So, given a pair of those files and a hex editor, I set about reverse engineering their contents - which turned out
not to be terribly hard, at least sufficiently to do _something_ useful with.

Details
-------

  * The `.opt` file is a vanilla ASCII CSV file with no headers, and contents that map document ids to be found in the
  `.dat` file to relative paths on disk, where the actual content lives.  Example line:
  `SOMECASE0000018,SOMECASEVOL001,\SOMECASEVOL001\IMAGES\001\SOMECASE0000018.jpg,Y,,,56` - I have no idea what the
  trailing fields are, but `SOMECASEVOL001` is the name-portion of the `.opt` and `.dat` files, and the first element
  is the name of a record referenced in the `.dat` file, so this is pretty clearly a file lookup table
  * The `.dat` file is CSV-like, with some quirks:
    * It starts with a [byte-order mark](https://en.wikipedia.org/wiki/Byte_order_mark)
    * Instead of commas, the field delimiter is `\u00fe\u0014\u00fe`
      * Lines are delimited by `CRLF`s.  In the data I have examined, it is always denote the end of a record (all
      records come up with the same number of fields as the headers, and the field names make sense - field names
      that denote a date wind up with data that is a date, etc.
         * It is sort of puzzling that someone would have been so concerned about byte-order and being able to
           encode commas, but completely unconcerned about being able to encode newlines - but, hey, it's software
           from the 80s and I shouldn't complain, since it made it easy to reverse-engineer
  * Paths in the files I have seen always use Windows' `\` as a delimiter - no idea if that is always the case

Status
------

This was developed to help out a friend, based on reverse-engineering a single dataset.  It was written in an
evening and tidied up in another.  If it works for you, great.  If you can improve it, submit a pull request.

### To-Dos

  * A lot of what this does could be done against a zip file _without unpacking it to disk_ - given the size of
  the data-sets, that might be a good idea, though it would slow things down.
  * It wouldn't be terribly hard to turn this into a NetBeans platform app, build a Lucene index of the data
   files where possible, and provide the ability to browse and search the raw data.

License
-------

I don't usually use the GPL, but given that the entire point of this project is to defeat lawyers behaving badly
and using cost-prohibitive proprietary software to screw anyone who can't afford it, the
[GNU Affero License](https://www.gnu.org/licenses/why-affero-gpl.en.html) seems only appropriate.  Use it,
contribute to it, but you *must* publish your improvements for the rest of the world to benefit from, period.
